{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e507061",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import torchaudio\n",
    "import torch\n",
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea35f695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the root folder path\n",
    "root_folder_path = \"C:\\\\Users\\\\User\\\\Apviza\\\\Project 6\\\\Output\"\n",
    "output_folder = \"C:\\\\Users\\\\User\\\\Apviza\\\\Project 6\\\\Model Data\\\\Altered\"\n",
    "\n",
    "# Initialize an empty list to store dataset dictionaries\n",
    "dataset_dict_list = []\n",
    "\n",
    "for root, _, _ in os.walk(root_folder_path):\n",
    "    for subfolder in os.listdir(root):\n",
    "        subfolder_path = os.path.join(root, subfolder)\n",
    "        if os.path.isdir(subfolder_path):\n",
    "            for sub_root, _, files in os.walk(subfolder_path):\n",
    "                for file in files:\n",
    "                    if file.endswith(\".wav\"):\n",
    "                        audio_path = os.path.join(sub_root, file)\n",
    "                        path_components = sub_root.split(os.path.sep)\n",
    "                        dialect_region = path_components[-2]\n",
    "                        speaker_id = path_components[-1]\n",
    "                        dataset_dict = {\n",
    "                            \"audio\": audio_path,\n",
    "                            \"dialect_region\": dialect_region,\n",
    "                            \"speaker_id\": speaker_id,\n",
    "                            \"audio_file\": file\n",
    "                        }\n",
    "                        dataset_dict_list.append(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9f7c1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in dataset_dict_list:\n",
    "    audio_file = item[\"audio_file\"]\n",
    "    speaker_id = item[\"speaker_id\"]\n",
    "    new_audio_name = f\"{speaker_id}_{audio_file}\"\n",
    "    new_audio_path = os.path.join(output_folder, new_audio_name)\n",
    "    \n",
    "    # Copy the audio file to the new path with the new name\n",
    "    shutil.copy(item[\"audio\"], new_audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c29dfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to the Altered and Pristine folders\n",
    "altered_folder = \"C:\\\\Users\\\\User\\\\Apviza\\\\Project 6\\\\Model Data\\\\Altered\"\n",
    "pristine_folder = \"C:\\\\Users\\\\User\\\\Apviza\\\\Project 6\\\\Model Data\\\\Pristine\"\n",
    "\n",
    "# Define paths for the train, test, and validation folders\n",
    "train_folder = \"C:\\\\Users\\\\User\\\\Apviza\\\\Project 6\\\\Model Data\\\\Train\"\n",
    "test_folder = \"C:\\\\Users\\\\User\\\\Apviza\\\\Project 6\\\\Model Data\\\\Test\"\n",
    "val_folder = \"C:\\\\Users\\\\User\\\\Apviza\\\\Project 6\\\\Model Data\\\\Validation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76dadd7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been split into train, test, and validation sets.\n"
     ]
    }
   ],
   "source": [
    "# Create train, test, and validation folders if they don't exist\n",
    "os.makedirs(train_folder, exist_ok=True)\n",
    "os.makedirs(test_folder, exist_ok=True)\n",
    "os.makedirs(val_folder, exist_ok=True)\n",
    "\n",
    "# Define the percentage split for train, test, and validation\n",
    "train_split = 0.7  # 70% for training\n",
    "test_split = 0.15  # 15% for testing\n",
    "val_split = 0.15  # 15% for validation\n",
    "\n",
    "# Iterate through the Altered folder\n",
    "for filename in os.listdir(altered_folder):\n",
    "    file_path = os.path.join(altered_folder, filename)\n",
    "    \n",
    "    # Randomly assign files to train, test, or validation based on the defined splits\n",
    "    rand = random.random()\n",
    "    if rand < train_split:\n",
    "        shutil.copy(file_path, os.path.join(train_folder, filename))\n",
    "    elif rand < train_split + test_split:\n",
    "        shutil.copy(file_path, os.path.join(test_folder, filename))\n",
    "    else:\n",
    "        shutil.copy(file_path, os.path.join(val_folder, filename))\n",
    "\n",
    "# Iterate through the Pristine folder\n",
    "for filename in os.listdir(pristine_folder):\n",
    "    file_path = os.path.join(pristine_folder, filename)\n",
    "    \n",
    "    # Randomly assign files to train, test, or validation based on the defined splits\n",
    "    rand = random.random()\n",
    "    if rand < train_split:\n",
    "        shutil.copy(file_path, os.path.join(train_folder, filename))\n",
    "    elif rand < train_split + test_split:\n",
    "        shutil.copy(file_path, os.path.join(test_folder, filename))\n",
    "    else:\n",
    "        shutil.copy(file_path, os.path.join(val_folder, filename))\n",
    "\n",
    "print(\"Data has been split into train, test, and validation sets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "808030eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1859328"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_root = \"C:\\\\Users\\\\User\\\\Apviza\\\\Project 6\\\\Model Data\"  # Change to your output folder path\n",
    "padded_root = \"C:\\\\Users\\\\User\\\\Apviza\\\\Project 6\\\\Padded\"  # New Padded folder\n",
    "subfolders = [\"train\", \"test\", \"validation\"]\n",
    "1859328\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8abf1770",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting MP3 to WAV for C:\\Users\\User\\Apviza\\Project 6\\Model Data\\train: 100%|██████████| 31013/31013 [1:35:53<00:00,  5.39it/s]\n",
      "Converting MP3 to WAV for C:\\Users\\User\\Apviza\\Project 6\\Model Data\\test: 100%|██████████| 6598/6598 [22:15<00:00,  4.94it/s]\n",
      "Converting MP3 to WAV for C:\\Users\\User\\Apviza\\Project 6\\Model Data\\validation: 100%|██████████| 6424/6424 [21:34<00:00,  4.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MP3 to WAV conversion completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "output_root = \"C:\\\\Users\\\\User\\\\Apviza\\\\Project 6\\\\Model Data\"  # Change to your output folder path\n",
    "subfolders = [\"train\", \"test\", \"validation\"]\n",
    "\n",
    "# Iterate through train, test, and validation subfolders\n",
    "for subfolder in subfolders:\n",
    "    src_folder = os.path.join(output_root, subfolder)\n",
    "    dest_folder = os.path.join(output_root, subfolder)\n",
    "\n",
    "    audio_files = [file for file in os.listdir(src_folder) if file.endswith(\".mp3\")]\n",
    "\n",
    "    for file in tqdm(audio_files, desc=f\"Converting MP3 to WAV for {src_folder}\"):\n",
    "        src_file = os.path.join(src_folder, file)\n",
    "        dest_file = os.path.join(dest_folder, file.replace(\".mp3\", \".wav\"))\n",
    "\n",
    "        # Convert .mp3 to .wav\n",
    "        audio = AudioSegment.from_mp3(src_file)\n",
    "        audio.export(dest_file, format=\"wav\")\n",
    "\n",
    "print(\"MP3 to WAV conversion completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10e0c8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing C:\\Users\\User\\Apviza\\Project 6\\Model Data\\train:  51%|█████     | 16957/33212 [00:01<00:01, 8522.38it/s]\n"
     ]
    },
    {
     "ename": "LibsndfileError",
     "evalue": "System error.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLibsndfileError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 49\u001b[0m\n\u001b[0;32m     46\u001b[0m     max_waveform_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1859328\u001b[39m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;66;03m# Pad and copy the audio files\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m     \u001b[43mpad_and_copy_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdest_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_waveform_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAudio padding and copying completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 36\u001b[0m, in \u001b[0;36mpad_and_copy_audio\u001b[1;34m(src_folder, dest_folder, max_length)\u001b[0m\n\u001b[0;32m     34\u001b[0m     padding \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m, pad_length), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32))\n\u001b[0;32m     35\u001b[0m     padded_waveform \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((waveform, padding), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 36\u001b[0m     \u001b[43mtorchaudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdest_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadded_waveform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     38\u001b[0m     torchaudio\u001b[38;5;241m.\u001b[39msave(dest_file, waveform, sample_rate)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch_env\\lib\\site-packages\\torchaudio\\backend\\soundfile_backend.py:451\u001b[0m, in \u001b[0;36msave\u001b[1;34m(filepath, src, sample_rate, channels_first, compression, format, encoding, bits_per_sample)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m channels_first:\n\u001b[0;32m    449\u001b[0m     src \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mt()\n\u001b[1;32m--> 451\u001b[0m \u001b[43msoundfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplerate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch_env\\lib\\site-packages\\soundfile.py:345\u001b[0m, in \u001b[0;36mwrite\u001b[1;34m(file, data, samplerate, subtype, endian, format, closefd)\u001b[0m\n\u001b[0;32m    342\u001b[0m     channels \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SoundFile(file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, samplerate, channels,\n\u001b[0;32m    344\u001b[0m                subtype, endian, \u001b[38;5;28mformat\u001b[39m, closefd) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m--> 345\u001b[0m     \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch_env\\lib\\site-packages\\soundfile.py:1020\u001b[0m, in \u001b[0;36mSoundFile.write\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1018\u001b[0m \u001b[38;5;66;03m# no copy is made if data has already the correct memory layout:\u001b[39;00m\n\u001b[0;32m   1019\u001b[0m data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mascontiguousarray(data)\n\u001b[1;32m-> 1020\u001b[0m written \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_array_io\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwrite\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m written \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(data)\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_frames(written)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch_env\\lib\\site-packages\\soundfile.py:1344\u001b[0m, in \u001b[0;36mSoundFile._array_io\u001b[1;34m(self, action, array, frames)\u001b[0m\n\u001b[0;32m   1342\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mitemsize \u001b[38;5;241m==\u001b[39m _ffi\u001b[38;5;241m.\u001b[39msizeof(ctype)\n\u001b[0;32m   1343\u001b[0m cdata \u001b[38;5;241m=\u001b[39m _ffi\u001b[38;5;241m.\u001b[39mcast(ctype \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m, array\u001b[38;5;241m.\u001b[39m__array_interface__[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m-> 1344\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cdata_io\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch_env\\lib\\site-packages\\soundfile.py:1354\u001b[0m, in \u001b[0;36mSoundFile._cdata_io\u001b[1;34m(self, action, data, ctype, frames)\u001b[0m\n\u001b[0;32m   1352\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_snd, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msf_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m action \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m ctype)\n\u001b[0;32m   1353\u001b[0m frames \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file, data, frames)\n\u001b[1;32m-> 1354\u001b[0m \u001b[43m_error_check\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_errorcode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseekable():\n\u001b[0;32m   1356\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseek(curr \u001b[38;5;241m+\u001b[39m frames, SEEK_SET)  \u001b[38;5;66;03m# Update read & write position\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch_env\\lib\\site-packages\\soundfile.py:1407\u001b[0m, in \u001b[0;36m_error_check\u001b[1;34m(err, prefix)\u001b[0m\n\u001b[0;32m   1405\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Raise LibsndfileError if there is an error.\"\"\"\u001b[39;00m\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1407\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LibsndfileError(err, prefix\u001b[38;5;241m=\u001b[39mprefix)\n",
      "\u001b[1;31mLibsndfileError\u001b[0m: System error."
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "output_root = \"C:\\\\Users\\\\User\\\\Apviza\\\\Project 6\\\\Model Data\"  # Change to your output folder path\n",
    "padded_root = \"C:\\\\Users\\\\User\\\\Apviza\\\\Project 6\\\\Truncated\"  # New Padded folder\n",
    "subfolders = [\"train\", \"test\", \"validation\"]\n",
    "\n",
    "# Create the Padded folder and subfolders\n",
    "os.makedirs(padded_root, exist_ok=True)\n",
    "for subfolder in subfolders:\n",
    "    os.makedirs(os.path.join(padded_root, subfolder), exist_ok=True)\n",
    "\n",
    "def resample_audio(waveform, target_sample_rate=16000):\n",
    "    audio = torchaudio.transforms.Resample(\n",
    "        orig_freq=waveform.shape[1], new_freq=target_sample_rate\n",
    "    )(waveform)\n",
    "    return audio\n",
    "\n",
    "# Define a function to pad and copy audio files\n",
    "def pad_and_copy_audio(src_folder, dest_folder, max_length):\n",
    "    audio_files = [file for file in os.listdir(src_folder) if file.endswith(\".wav\")]\n",
    "\n",
    "    for file in tqdm(audio_files, desc=f\"Processing {src_folder}\"):\n",
    "        src_file = os.path.join(src_folder, file)\n",
    "        dest_file = os.path.join(dest_folder, file)\n",
    "\n",
    "        # Check if the destination file already exists, and skip if it does\n",
    "        if os.path.exists(dest_file):\n",
    "            continue\n",
    "\n",
    "        waveform, sample_rate = torchaudio.load(src_file)\n",
    "\n",
    "        # Pad the waveform if needed\n",
    "        pad_length = max_length - waveform.size(1)\n",
    "        if pad_length > 0:\n",
    "            padding = torch.from_numpy(np.zeros((1, pad_length), dtype=np.float32))\n",
    "            padded_waveform = torch.cat((waveform, padding), dim=1)\n",
    "            torchaudio.save(dest_file, padded_waveform, sample_rate)\n",
    "        else:\n",
    "            torchaudio.save(dest_file, waveform, sample_rate)\n",
    "\n",
    "# Iterate through train, test, and validation subfolders\n",
    "for subfolder in subfolders:\n",
    "    src_folder = os.path.join(output_root, subfolder)\n",
    "    dest_folder = os.path.join(padded_root, subfolder)\n",
    "\n",
    "    # Determine the maximum waveform length in this dataset\n",
    "    max_waveform_length = 23296\n",
    "\n",
    "    # Pad and copy the audio files\n",
    "    pad_and_copy_audio(src_folder, dest_folder, max_waveform_length)\n",
    "\n",
    "print(\"Audio padding and copying completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19d531d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest waveform length: 23296\n"
     ]
    }
   ],
   "source": [
    "output_root = \"C:\\\\Users\\\\User\\\\Apviza\\\\Project 6\\\\Model Data\"\n",
    "subfolders = [\"train\", \"test\", \"validation\"]\n",
    "\n",
    "# Initialize a variable to store the smallest waveform length\n",
    "smallest_waveform_length = float('inf')\n",
    "\n",
    "for subfolder in subfolders:\n",
    "    folder_path = os.path.join(output_root, subfolder)\n",
    "    audio_files = [file for file in os.listdir(folder_path) if file.endswith(\".wav\")]\n",
    "\n",
    "    for file in audio_files:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        waveform, _ = torchaudio.load(file_path)\n",
    "        waveform_length = waveform.size(1)\n",
    "        \n",
    "        if waveform_length < smallest_waveform_length:\n",
    "            smallest_waveform_length = waveform_length\n",
    "\n",
    "print(\"Smallest waveform length:\", smallest_waveform_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b52e3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing C:\\Users\\User\\Apviza\\Project 6\\Model Data\\train: 100%|██████████| 33212/33212 [02:58<00:00, 186.32it/s]\n",
      "Processing C:\\Users\\User\\Apviza\\Project 6\\Model Data\\test: 100%|██████████| 7057/7057 [00:37<00:00, 186.07it/s]\n",
      "Processing C:\\Users\\User\\Apviza\\Project 6\\Model Data\\validation: 100%|██████████| 6892/6892 [00:38<00:00, 180.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio truncating and copying completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output_root = \"C:\\\\Users\\\\User\\\\Apviza\\\\Project 6\\\\Model Data\"  # Change to your output folder path\n",
    "truncated_root = \"C:\\\\Users\\\\User\\\\Apviza\\\\Project 6\\\\Truncated\"  # New Truncated folder\n",
    "subfolders = [\"train\", \"test\", \"validation\"]\n",
    "\n",
    "# Create the Truncated folder and subfolders\n",
    "os.makedirs(truncated_root, exist_ok=True)\n",
    "for subfolder in subfolders:\n",
    "    os.makedirs(os.path.join(truncated_root, subfolder), exist_ok=True)\n",
    "\n",
    "# Define a function to truncate audio files\n",
    "def truncate_audio(src_folder, dest_folder, max_length):\n",
    "    audio_files = [file for file in os.listdir(src_folder) if file.endswith(\".wav\")]\n",
    "\n",
    "    for file in tqdm(audio_files, desc=f\"Processing {src_folder}\"):\n",
    "        src_file = os.path.join(src_folder, file)\n",
    "        dest_file = os.path.join(dest_folder, file)\n",
    "\n",
    "        # Check if the destination file already exists, and skip if it does\n",
    "        if os.path.exists(dest_file):\n",
    "            continue\n",
    "\n",
    "        waveform, sample_rate = torchaudio.load(src_file)\n",
    "\n",
    "        # Truncate the waveform if needed\n",
    "        if waveform.size(1) > max_length:\n",
    "            truncated_waveform = waveform[:, :max_length]\n",
    "            torchaudio.save(dest_file, truncated_waveform, sample_rate)\n",
    "        else:\n",
    "            torchaudio.save(dest_file, waveform, sample_rate)\n",
    "\n",
    "# Iterate through train, test, and validation subfolders\n",
    "for subfolder in subfolders:\n",
    "    src_folder = os.path.join(output_root, subfolder)\n",
    "    dest_folder = os.path.join(truncated_root, subfolder)\n",
    "\n",
    "    # Determine the maximum waveform length in this dataset\n",
    "    max_waveform_length = 23296\n",
    "\n",
    "    # Truncate and copy the audio files\n",
    "    truncate_audio(src_folder, dest_folder, max_waveform_length)\n",
    "\n",
    "print(\"Audio truncating and copying completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de965be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch_env] *",
   "language": "python",
   "name": "conda-env-torch_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
