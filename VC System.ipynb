{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57ed26d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "import phonemizer\n",
    "from phonemizer import phonemize\n",
    "from datasets import load_dataset\n",
    "from scipy.io import wavfile\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de032212",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phonemizer.backend.espeak.wrapper import EspeakWrapper\n",
    "_ESPEAK_LIBRARY = 'C:\\Program Files\\eSpeak NG\\libespeak-ng.dll'\n",
    "EspeakWrapper.set_library(_ESPEAK_LIBRARY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e26c885",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset timit_asr (C:/Users/User/.cache/huggingface/datasets/timit_asr/default-63b686f1413ccacf/0.0.0/43f9448dd5db58e95ee48a277f466481b151f112ea53e27f8173784da9254fb2)\n",
      "100%|██████████| 2/2 [00:00<00:00, 95.49it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"timit_asr\",data_dir='C:\\\\Users\\\\User\\\\Apviza\\\\Project 6\\\\TIMIT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0824a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\torch_env\\lib\\site-packages\\torch\\hub.py:286: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n",
      "  warnings.warn(\n",
      "Using cache found in C:\\Users\\User/.cache\\torch\\hub\\nvidia_DeepLearningExamples_torchhub\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tacotron2(\n",
       "  (embedding): Embedding(148, 512)\n",
       "  (encoder): Encoder(\n",
       "    (convolutions): ModuleList(\n",
       "      (0-2): 3 x Sequential(\n",
       "        (0): ConvNorm(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        )\n",
       "        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (lstm): LSTM(512, 256, batch_first=True, bidirectional=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (prenet): Prenet(\n",
       "      (layers): ModuleList(\n",
       "        (0): LinearNorm(\n",
       "          (linear_layer): Linear(in_features=80, out_features=256, bias=False)\n",
       "        )\n",
       "        (1): LinearNorm(\n",
       "          (linear_layer): Linear(in_features=256, out_features=256, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (attention_rnn): LSTMCell(768, 1024)\n",
       "    (attention_layer): Attention(\n",
       "      (query_layer): LinearNorm(\n",
       "        (linear_layer): Linear(in_features=1024, out_features=128, bias=False)\n",
       "      )\n",
       "      (memory_layer): LinearNorm(\n",
       "        (linear_layer): Linear(in_features=512, out_features=128, bias=False)\n",
       "      )\n",
       "      (v): LinearNorm(\n",
       "        (linear_layer): Linear(in_features=128, out_features=1, bias=False)\n",
       "      )\n",
       "      (location_layer): LocationLayer(\n",
       "        (location_conv): ConvNorm(\n",
       "          (conv): Conv1d(2, 32, kernel_size=(31,), stride=(1,), padding=(15,), bias=False)\n",
       "        )\n",
       "        (location_dense): LinearNorm(\n",
       "          (linear_layer): Linear(in_features=32, out_features=128, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder_rnn): LSTMCell(1536, 1024, bias=1)\n",
       "    (linear_projection): LinearNorm(\n",
       "      (linear_layer): Linear(in_features=1536, out_features=80, bias=True)\n",
       "    )\n",
       "    (gate_layer): LinearNorm(\n",
       "      (linear_layer): Linear(in_features=1536, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (postnet): Postnet(\n",
       "    (convolutions): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): ConvNorm(\n",
       "          (conv): Conv1d(80, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        )\n",
       "        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1-3): 3 x Sequential(\n",
       "        (0): ConvNorm(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        )\n",
       "        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): ConvNorm(\n",
       "          (conv): Conv1d(512, 80, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        )\n",
       "        (1): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Tacotron2 models\n",
    "tacotron2 = torch.hub.load('nvidia/DeepLearningExamples:torchhub', 'nvidia_tacotron2')\n",
    "tacotron2 = tacotron2.to('cuda')\n",
    "tacotron2.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a58a7a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\User/.cache\\torch\\hub\\nvidia_DeepLearningExamples_torchhub\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WaveGlow(\n",
       "  (upsample): ConvTranspose1d(80, 80, kernel_size=(1024,), stride=(256,))\n",
       "  (WN): ModuleList(\n",
       "    (0-3): 4 x WN(\n",
       "      (in_layers): ModuleList(\n",
       "        (0): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (1): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "        (2): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))\n",
       "        (3): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))\n",
       "        (4): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(16,))\n",
       "        (5): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(32,))\n",
       "        (6): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(64,))\n",
       "        (7): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(128,))\n",
       "      )\n",
       "      (res_skip_layers): ModuleList(\n",
       "        (0-6): 7 x Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
       "        (7): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (cond_layers): ModuleList(\n",
       "        (0-7): 8 x Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (start): Conv1d(4, 512, kernel_size=(1,), stride=(1,))\n",
       "      (end): Conv1d(512, 8, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "    (4-7): 4 x WN(\n",
       "      (in_layers): ModuleList(\n",
       "        (0): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (1): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "        (2): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))\n",
       "        (3): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))\n",
       "        (4): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(16,))\n",
       "        (5): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(32,))\n",
       "        (6): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(64,))\n",
       "        (7): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(128,))\n",
       "      )\n",
       "      (res_skip_layers): ModuleList(\n",
       "        (0-6): 7 x Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
       "        (7): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (cond_layers): ModuleList(\n",
       "        (0-7): 8 x Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (start): Conv1d(3, 512, kernel_size=(1,), stride=(1,))\n",
       "      (end): Conv1d(512, 6, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "    (8-11): 4 x WN(\n",
       "      (in_layers): ModuleList(\n",
       "        (0): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (1): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "        (2): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))\n",
       "        (3): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))\n",
       "        (4): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(16,))\n",
       "        (5): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(32,))\n",
       "        (6): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(64,))\n",
       "        (7): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(128,))\n",
       "      )\n",
       "      (res_skip_layers): ModuleList(\n",
       "        (0-6): 7 x Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
       "        (7): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (cond_layers): ModuleList(\n",
       "        (0-7): 8 x Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (start): Conv1d(2, 512, kernel_size=(1,), stride=(1,))\n",
       "      (end): Conv1d(512, 4, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "  )\n",
       "  (convinv): ModuleList(\n",
       "    (0-3): 4 x Invertible1x1Conv(\n",
       "      (conv): Conv1d(8, 8, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    )\n",
       "    (4-7): 4 x Invertible1x1Conv(\n",
       "      (conv): Conv1d(6, 6, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    )\n",
       "    (8-11): 4 x Invertible1x1Conv(\n",
       "      (conv): Conv1d(4, 4, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load WaveGlow models\n",
    "waveglow = torch.hub.load('nvidia/DeepLearningExamples:torchhub', 'nvidia_waveglow')\n",
    "waveglow = waveglow.remove_weightnorm(waveglow)\n",
    "waveglow = waveglow.to('cuda')\n",
    "waveglow.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95eec492",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\User/.cache\\torch\\hub\\NVIDIA_DeepLearningExamples_torchhub\n"
     ]
    }
   ],
   "source": [
    "# preprocessing\n",
    "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tts_utils')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27cb007c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesize_text(text):\n",
    "    sequences, lengths = utils.prepare_input_sequence([text])\n",
    "    with torch.no_grad():\n",
    "        mel, _, _ = tacotron2.infer(sequences, lengths)\n",
    "        audio = waveglow.infer(mel)\n",
    "        audio_numpy = audio[0].data.cpu().numpy()\n",
    "    return audio_numpy\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dac2b8be",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m output_path \u001b[38;5;241m=\u001b[39m Path(output_dir) \u001b[38;5;241m/\u001b[39m dialect_region \u001b[38;5;241m/\u001b[39m speaker_id\n\u001b[0;32m     13\u001b[0m output_path\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 15\u001b[0m audio \u001b[38;5;241m=\u001b[39m \u001b[43msynthesize_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m output_file \u001b[38;5;241m=\u001b[39m output_path \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mid_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     18\u001b[0m wavfile\u001b[38;5;241m.\u001b[39mwrite(output_file, \u001b[38;5;241m22050\u001b[39m, audio)\n",
      "Cell \u001b[1;32mIn[17], line 6\u001b[0m, in \u001b[0;36msynthesize_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      4\u001b[0m     mel, _, _ \u001b[38;5;241m=\u001b[39m tacotron2\u001b[38;5;241m.\u001b[39minfer(sequences, lengths)\n\u001b[0;32m      5\u001b[0m     audio \u001b[38;5;241m=\u001b[39m waveglow\u001b[38;5;241m.\u001b[39minfer(mel)\n\u001b[1;32m----> 6\u001b[0m     audio_numpy \u001b[38;5;241m=\u001b[39m \u001b[43maudio\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m audio_numpy\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "output_dir = \"Output\"\n",
    "\n",
    "for split in [\"train\", \"test\"]:\n",
    "    examples = dataset[split]\n",
    "    for example in examples:\n",
    "        text = example[\"text\"]\n",
    "        dialect_region = example[\"dialect_region\"]\n",
    "        speaker_id = example[\"speaker_id\"]\n",
    "        id_ = example[\"id\"]\n",
    "        \n",
    "        # Create output directory structure\n",
    "        output_path = Path(output_dir) / dialect_region / speaker_id\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        audio = synthesize_text(text)\n",
    "        \n",
    "        output_file = output_path / f\"{id_}.wav\"\n",
    "        wavfile.write(output_file, 22050, audio)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
